# é›†æˆç¤ºä¾‹

è¿™äº›ç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•å°† AI Tech Innovation Suite çš„å„ä¸ªç»„ä»¶é›†æˆåœ¨ä¸€èµ·ï¼Œä»¥åŠä¸ç¬¬ä¸‰æ–¹æœåŠ¡çš„é›†æˆã€‚

## ç›®å½•

- [å¤šå¹³å°å†…å®¹å‘å¸ƒ](#å¤šå¹³å°å†…å®¹å‘å¸ƒ)
- [çŸ¥è¯†å›¾è°±æ„å»º](#çŸ¥è¯†å›¾è°±æ„å»º)
- [å®æ—¶åˆ†æç³»ç»Ÿ](#å®æ—¶åˆ†æç³»ç»Ÿ)
- [ç¬¬ä¸‰æ–¹APIé›†æˆ](#ç¬¬ä¸‰æ–¹apié›†æˆ)

## å¤šå¹³å°å†…å®¹å‘å¸ƒ

å±•ç¤ºå¦‚ä½•å°†å°çº¢ä¹¦Agentä¸å…¶ä»–å¹³å°çš„å†…å®¹å‘å¸ƒç³»ç»Ÿé›†æˆã€‚

```python
# examples/integrations/multi_platform_publisher.py
import asyncio
from typing import List, Dict, Any
from packages.xiaohongshu_agent import XiaohongshuAgent
from packages.scheduler_agent import SchedulerAgent

class MultiPlatformPublisher:
    """å¤šå¹³å°å†…å®¹å‘å¸ƒå™¨"""
    
    def __init__(self):
        self.xiaohongshu_agent = XiaohongshuAgent()
        self.scheduler = SchedulerAgent()
        self.platform_configs = {
            "xiaohongshu": {
                "enabled": True,
                "api_config": "xiaohongshu_config",
                "content_format": "xiaohongshu_style"
            },
            "weibo": {
                "enabled": False,  # ç¤ºä¾‹ä¸­ç¦ç”¨
                "api_config": "weibo_config", 
                "content_format": "weibo_style"
            },
            "douyin": {
                "enabled": False,  # ç¤ºä¾‹ä¸­ç¦ç”¨
                "api_config": "douyin_config",
                "content_format": "douyin_style"
            }
        }
    
    async def create_multi_platform_campaign(self, campaign_data: Dict[str, Any]):
        """åˆ›å»ºå¤šå¹³å°è¥é”€æ´»åŠ¨"""
        print(f"ğŸš€ åˆ›å»ºå¤šå¹³å°è¥é”€æ´»åŠ¨: {campaign_data['name']}")
        
        # ä¸ºæ¯ä¸ªå¯ç”¨çš„å¹³å°ç”Ÿæˆå†…å®¹
        platform_contents = {}
        
        for platform, config in self.platform_configs.items():
            if not config["enabled"]:
                continue
                
            print(f"ğŸ“± ä¸ºå¹³å° {platform} ç”Ÿæˆå†…å®¹...")
            
            if platform == "xiaohongshu":
                content = await self._generate_xiaohongshu_content(campaign_data)
            else:
                # å…¶ä»–å¹³å°çš„ç”Ÿæˆé€»è¾‘
                content = await self._generate_platform_content(platform, campaign_data)
            
            platform_contents[platform] = content
        
        # è°ƒåº¦å‘å¸ƒ
        scheduled_tasks = await self._schedule_publication(platform_contents, campaign_data)
        
        return {
            "campaign_id": campaign_data["id"],
            "platform_contents": platform_contents,
            "scheduled_tasks": scheduled_tasks,
            "status": "scheduled"
        }
    
    async def _generate_xiaohongshu_content(self, campaign_data: Dict[str, Any]):
        """ç”Ÿæˆå°çº¢ä¹¦å†…å®¹"""
        content = await self.xiaohongshu_agent.generate_content(
            topic=campaign_data["topic"],
            brand_info=campaign_data["brand_info"],
            content_type=campaign_data.get("content_type", "product_promotion"),
            additional_context={
                "target_audience": campaign_data.get("target_audience"),
                "key_messages": campaign_data.get("key_messages", []),
                "hashtag_strategy": campaign_data.get("hashtag_strategy")
            }
        )
        
        # æ ¹æ®å¹³å°ç‰¹æ€§è°ƒæ•´å†…å®¹
        content["platform"] = "xiaohongshu"
        content["character_limit"] = 2200  # å°çº¢ä¹¦å­—ç¬¦é™åˆ¶
        content["image_requirements"] = {
            "aspect_ratio": "1:1",
            "max_size": (1080, 1080),
            "format": "JPG"
        }
        
        return content
    
    async def _generate_platform_content(self, platform: str, campaign_data: Dict[str, Any]):
        """ä¸ºå…¶ä»–å¹³å°ç”Ÿæˆå†…å®¹"""
        # è¿™é‡Œå¯ä»¥å®ç°å…¶ä»–å¹³å°çš„å†…å®¹ç”Ÿæˆé€»è¾‘
        # ä¾‹å¦‚å¾®åšã€æŠ–éŸ³ç­‰
        
        base_content = {
            "platform": platform,
            "title": f"{campaign_data['topic']} - {platform}",
            "content": f"åœ¨{platform}ä¸Šåˆ†äº«å…³äº{campaign_data['topic']}çš„å†…å®¹",
            "hashtags": [f"#{campaign_data['topic']}", f"#{platform}"],
            "character_limit": 140 if platform == "weibo" else 2200,
            "image_requirements": {
                "aspect_ratio": "16:9" if platform == "weibo" else "1:1",
                "max_size": (1080, 1080),
                "format": "JPG"
            }
        }
        
        return base_content
    
    async def _schedule_publication(self, platform_contents: Dict[str, Any], campaign_data: Dict[str, Any]):
        """è°ƒåº¦å†…å®¹å‘å¸ƒ"""
        scheduled_tasks = []
        
        for platform, content in platform_contents.items():
            # è®¡ç®—æœ€ä½³å‘å¸ƒæ—¶é—´
            optimal_time = await self._calculate_optimal_time(platform, campaign_data)
            
            # åˆ›å»ºè°ƒåº¦ä»»åŠ¡
            task = await self.scheduler.schedule_task(
                task_type="content_publication",
                target_time=optimal_time,
                task_data={
                    "platform": platform,
                    "content": content,
                    "campaign_id": campaign_data["id"]
                },
                priority=campaign_data.get("priority", "normal")
            )
            
            scheduled_tasks.append({
                "platform": platform,
                "task_id": task["task_id"],
                "scheduled_time": optimal_time,
                "status": "scheduled"
            })
            
            print(f"âœ… {platform} å†…å®¹å·²è°ƒåº¦åˆ° {optimal_time}")
        
        return scheduled_tasks
    
    async def _calculate_optimal_time(self, platform: str, campaign_data: Dict[str, Any]):
        """è®¡ç®—æœ€ä½³å‘å¸ƒæ—¶é—´"""
        # ç®€å•çš„æœ€ä½³æ—¶é—´è®¡ç®—é€»è¾‘
        # å®é™…åº”ç”¨ä¸­å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„ç®—æ³•
        
        from datetime import datetime, timedelta
        
        base_time = datetime.now()
        
        # å¹³å°ç‰¹å®šçš„å‘å¸ƒæ—¶é—´ç­–ç•¥
        platform_strategies = {
            "xiaohongshu": {"hour": 20, "minute": 0},  # æ™šä¸Š8ç‚¹
            "weibo": {"hour": 12, "minute": 0},        # ä¸­åˆ12ç‚¹
            "douyin": {"hour": 19, "minute": 30}       # æ™šä¸Š7ç‚¹åŠ
        }
        
        strategy = platform_strategies.get(platform, {"hour": 10, "minute": 0})
        
        # è®¾ç½®ä¸ºä»Šå¤©çš„æœ€ä½³æ—¶é—´ï¼Œå¦‚æœå·²è¿‡åˆ™è®¾ä¸ºæ˜å¤©
        optimal_time = base_time.replace(
            hour=strategy["hour"],
            minute=strategy["minute"],
            second=0,
            microsecond=0
        )
        
        if optimal_time <= base_time:
            optimal_time += timedelta(days=1)
        
        return optimal_time
    
    async def sync_content_performance(self, campaign_id: str):
        """åŒæ­¥å†…å®¹è¡¨ç°æ•°æ®"""
        print(f"ğŸ“Š åŒæ­¥æ´»åŠ¨ {campaign_id} çš„è¡¨ç°æ•°æ®...")
        
        performance_data = {}
        
        for platform in self.platform_configs:
            if not self.platform_configs[platform]["enabled"]:
                continue
            
            # è·å–å¹³å°è¡¨ç°æ•°æ®
            platform_data = await self._fetch_platform_metrics(platform, campaign_id)
            performance_data[platform] = platform_data
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        report = await self._generate_performance_report(performance_data)
        
        return report
    
    async def _fetch_platform_metrics(self, platform: str, campaign_id: str):
        """è·å–å¹³å°æŒ‡æ ‡"""
        # æ¨¡æ‹ŸAPIè°ƒç”¨è·å–å¹³å°æ•°æ®
        # å®é™…åº”ç”¨ä¸­éœ€è¦è°ƒç”¨å„å¹³å°çš„API
        
        return {
            "platform": platform,
            "campaign_id": campaign_id,
            "views": 1000,
            "likes": 150,
            "comments": 25,
            "shares": 12,
            "engagement_rate": 0.187,
            "reach": 850,
            "impressions": 1200
        }
    
    async def _generate_performance_report(self, performance_data: Dict[str, Any]):
        """ç”Ÿæˆè¡¨ç°æŠ¥å‘Š"""
        total_views = sum(data["views"] for data in performance_data.values())
        total_likes = sum(data["likes"] for data in performance_data.values())
        total_comments = sum(data["comments"] for data in performance_data.values())
        
        report = {
            "summary": {
                "total_platforms": len(performance_data),
                "total_views": total_views,
                "total_likes": total_likes,
                "total_comments": total_comments,
                "overall_engagement": (total_likes + total_comments) / total_views if total_views > 0 else 0
            },
            "platform_breakdown": performance_data,
            "recommendations": await self._generate_recommendations(performance_data)
        }
        
        return report
    
    async def _generate_recommendations(self, performance_data: Dict[str, Any]):
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        for platform, data in performance_data.items():
            if data["engagement_rate"] < 0.1:
                recommendations.append({
                    "platform": platform,
                    "type": "engagement_improvement",
                    "suggestion": f"æé«˜{platform}ä¸Šçš„äº’åŠ¨ç‡ï¼Œè€ƒè™‘è°ƒæ•´å†…å®¹ç­–ç•¥"
                })
            
            if data["views"] < 500:
                recommendations.append({
                    "platform": platform,
                    "type": "reach_improvement", 
                    "suggestion": f"å¢åŠ {platform}çš„æ›å…‰é‡ï¼Œè€ƒè™‘ä½¿ç”¨ä»˜è´¹æ¨å¹¿"
                })
        
        return recommendations
    
    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        await self.xiaohongshu_agent.cleanup()
        await self.scheduler.cleanup()

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    publisher = MultiPlatformPublisher()
    
    try:
        # åˆ›å»ºè¥é”€æ´»åŠ¨
        campaign_data = {
            "id": "campaign_001",
            "name": "å¤æ—¥æŠ¤è‚¤æ–°å“æ¨å¹¿",
            "topic": "å¤æ—¥æŠ¤è‚¤å¿ƒå¾—",
            "brand_info": {
                "name": "ç¾ä¸½æ—¥è®°",
                "category": "æŠ¤è‚¤",
                "target_audience": "18-35å²å¥³æ€§"
            },
            "content_type": "product_promotion",
            "key_messages": ["å¤©ç„¶æˆåˆ†", "æ¸©å’Œæ— åˆºæ¿€", "å¤æ—¥ä¸“ç”¨"],
            "hashtag_strategy": ["#æŠ¤è‚¤", "#å¤æ—¥", "#ç¾ä¸½æ—¥è®°"],
            "priority": "high",
            "budget": 10000
        }
        
        # åˆ›å»ºå¤šå¹³å°æ´»åŠ¨
        result = await publisher.create_multi_platform_campaign(campaign_data)
        print(f"æ´»åŠ¨åˆ›å»ºç»“æœ: {result['status']}")
        
        # ç­‰å¾…ä¸€æ®µæ—¶é—´ååŒæ­¥è¡¨ç°æ•°æ®
        print("â³ ç­‰å¾…24å°æ—¶ä»¥æ”¶é›†è¡¨ç°æ•°æ®...")
        await asyncio.sleep(2)  # å®é™…åº”ç”¨ä¸­ç­‰å¾…24å°æ—¶
        
        # åŒæ­¥è¡¨ç°æ•°æ®
        performance_report = await publisher.sync_content_performance(campaign_data["id"])
        print(f"è¡¨ç°æŠ¥å‘Š: {performance_report['summary']}")
        
    finally:
        await publisher.cleanup()

if __name__ == "__main__":
    asyncio.run(main())
```

## çŸ¥è¯†å›¾è°±æ„å»º

å±•ç¤ºå¦‚ä½•æ„å»ºå’Œåˆ†æçŸ¥è¯†å›¾è°±ã€‚

```python
# examples/integrations/knowledge_graph_builder.py
import asyncio
from typing import List, Dict, Any, Set
from packages.knowledge_emergence import KnowledgeEmergenceAnalyzer
from packages.knowledge_management import KnowledgeManager

class KnowledgeGraphBuilder:
    """çŸ¥è¯†å›¾è°±æ„å»ºå™¨"""
    
    def __init__(self):
        self.knowledge_analyzer = KnowledgeEmergenceAnalyzer()
        self.knowledge_manager = KnowledgeManager()
        self.graph_data = {
            "nodes": [],
            "edges": [],
            "metadata": {}
        }
    
    async def build_knowledge_graph(self, source_documents: List[Dict[str, Any]]):
        """ä»æ–‡æ¡£æ„å»ºçŸ¥è¯†å›¾è°±"""
        print(f"ğŸ” å¼€å§‹æ„å»ºçŸ¥è¯†å›¾è°±ï¼Œå¤„ç† {len(source_documents)} ä¸ªæ–‡æ¡£...")
        
        # ç¬¬ä¸€æ­¥ï¼šæå–å®ä½“
        entities = await self._extract_entities(source_documents)
        print(f"ğŸ“ æå–åˆ° {len(entities)} ä¸ªå®ä½“")
        
        # ç¬¬äºŒæ­¥ï¼šè¯†åˆ«å…³ç³»
        relationships = await self._extract_relationships(source_documents, entities)
        print(f"ğŸ”— è¯†åˆ«åˆ° {len(relationships)} ä¸ªå…³ç³»")
        
        # ç¬¬ä¸‰æ­¥ï¼šæ„å»ºå›¾è°±
        await self._construct_graph(entities, relationships)
        
        # ç¬¬å››æ­¥ï¼šè®¡ç®—å›¾è°±æŒ‡æ ‡
        metrics = await self._calculate_graph_metrics()
        
        # ç¬¬äº”æ­¥ï¼šç”Ÿæˆå¯è§†åŒ–
        visualization = await self._generate_visualization()
        
        return {
            "graph_data": self.graph_data,
            "metrics": metrics,
            "visualization": visualization,
            "statistics": {
                "total_nodes": len(self.graph_data["nodes"]),
                "total_edges": len(self.graph_data["edges"]),
                "density": self._calculate_density(),
                "connected_components": self._find_connected_components()
            }
        }
    
    async def _extract_entities(self, documents: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æå–å®ä½“"""
        entities = []
        
        for doc in documents:
            # ä½¿ç”¨çŸ¥è¯†åˆ†æå™¨æå–å®ä½“
            doc_entities = await self.knowledge_analyzer.extract_entities(doc["content"])
            
            for entity in doc_entities:
                entity_data = {
                    "id": f"entity_{len(entities)}",
                    "name": entity["text"],
                    "type": entity["type"],
                    "confidence": entity["confidence"],
                    "source_document": doc.get("id", "unknown"),
                    "context": entity.get("context", ""),
                    "properties": entity.get("properties", {})
                }
                entities.append(entity_data)
        
        # å»é‡å’Œåˆå¹¶ç›¸ä¼¼å®ä½“
        merged_entities = await self._merge_similar_entities(entities)
        return merged_entities
    
    async def _extract_relationships(self, documents: List[Dict[str, Any]], entities: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æå–å…³ç³»"""
        relationships = []
        
        for doc in documents:
            # ä½¿ç”¨çŸ¥è¯†åˆ†æå™¨æå–å…³ç³»
            doc_relationships = await self.knowledge_analyzer.extract_relationships(
                doc["content"], entities
            )
            
            for rel in doc_relationships:
                relationship_data = {
                    "id": f"rel_{len(relationships)}",
                    "source": rel["source_entity"],
                    "target": rel["target_entity"],
                    "relation_type": rel["type"],
                    "confidence": rel["confidence"],
                    "source_document": doc.get("id", "unknown"),
                    "context": rel.get("context", ""),
                    "properties": rel.get("properties", {})
                }
                relationships.append(relationship_data)
        
        return relationships
    
    async def _merge_similar_entities(self, entities: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """åˆå¹¶ç›¸ä¼¼å®ä½“"""
        merged = []
        processed = set()
        
        for i, entity in enumerate(entities):
            if i in processed:
                continue
            
            # æŸ¥æ‰¾ç›¸ä¼¼å®ä½“
            similar_entities = [entity]
            for j, other_entity in enumerate(entities[i+1:], i+1):
                if j in processed:
                    continue
                
                if self._are_entities_similar(entity, other_entity):
                    similar_entities.append(other_entity)
                    processed.add(j)
            
            # åˆå¹¶ç›¸ä¼¼å®ä½“
            merged_entity = self._merge_entity_group(similar_entities)
            merged.append(merged_entity)
            processed.add(i)
        
        return merged
    
    def _are_entities_similar(self, entity1: Dict[str, Any], entity2: Dict[str, Any]) -> bool:
        """åˆ¤æ–­å®ä½“æ˜¯å¦ç›¸ä¼¼"""
        # ç®€å•çš„ç›¸ä¼¼æ€§åˆ¤æ–­é€»è¾‘
        # å®é™…åº”ç”¨ä¸­å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„ç®—æ³•
        
        if entity1["type"] != entity2["type"]:
            return False
        
        # æ£€æŸ¥åç§°ç›¸ä¼¼æ€§
        name1 = entity1["name"].lower()
        name2 = entity2["name"].lower()
        
        # å®Œå…¨åŒ¹é…
        if name1 == name2:
            return True
        
        # éƒ¨åˆ†åŒ¹é…
        if name1 in name2 or name2 in name1:
            return True
        
        # åŒ…å«å…±åŒå…³é”®è¯
        words1 = set(name1.split())
        words2 = set(name2.split())
        if len(words1 & words2) / len(words1 | words2) > 0.7:
            return True
        
        return False
    
    def _merge_entity_group(self, entities: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆå¹¶å®ä½“ç»„"""
        # é€‰æ‹©ç½®ä¿¡åº¦æœ€é«˜çš„ä½œä¸ºä¸»å®ä½“
        main_entity = max(entities, key=lambda x: x["confidence"])
        
        # åˆå¹¶å±æ€§
        merged_properties = {}
        all_sources = []
        
        for entity in entities:
            merged_properties.update(entity["properties"])
            all_sources.append(entity["source_document"])
        
        return {
            "id": main_entity["id"],
            "name": main_entity["name"],
            "type": main_entity["type"],
            "confidence": main_entity["confidence"],
            "source_documents": list(set(all_sources)),
            "context": main_entity["context"],
            "properties": merged_properties,
            "merged_from": [e["id"] for e in entities]
        }
    
    async def _construct_graph(self, entities: List[Dict[str, Any]], relationships: List[Dict[str, Any]]):
        """æ„å»ºå›¾è°±"""
        # æ·»åŠ èŠ‚ç‚¹
        self.graph_data["nodes"] = entities
        
        # æ·»åŠ è¾¹
        self.graph_data["edges"] = relationships
        
        # è®¾ç½®å…ƒæ•°æ®
        self.graph_data["metadata"] = {
            "created_at": asyncio.get_event_loop().time(),
            "entity_count": len(entities),
            "relationship_count": len(relationships),
            "source_documents": list(set(
                entity["source_document"] for entity in entities
            ))
        }
    
    async def _calculate_graph_metrics(self) -> Dict[str, Any]:
        """è®¡ç®—å›¾è°±æŒ‡æ ‡"""
        metrics = {
            "node_metrics": await self._calculate_node_metrics(),
            "edge_metrics": await self._calculate_edge_metrics(),
            "graph_metrics": await self._calculate_global_metrics()
        }
        
        return metrics
    
    async def _calculate_node_metrics(self) -> Dict[str, Any]:
        """è®¡ç®—èŠ‚ç‚¹æŒ‡æ ‡"""
        node_types = {}
        centrality_scores = {}
        
        for node in self.graph_data["nodes"]:
            # ç»Ÿè®¡èŠ‚ç‚¹ç±»å‹
            node_type = node["type"]
            node_types[node_type] = node_types.get(node_type, 0) + 1
            
            # è®¡ç®—åº¦ä¸­å¿ƒæ€§
            degree = self._calculate_node_degree(node["id"])
            centrality_scores[node["id"]] = degree
        
        return {
            "type_distribution": node_types,
            "centrality_scores": centrality_scores,
            "top_central_nodes": sorted(
                centrality_scores.items(),
                key=lambda x: x[1],
                reverse=True
            )[:10]
        }
    
    async def _calculate_edge_metrics(self) -> Dict[str, Any]:
        """è®¡ç®—è¾¹æŒ‡æ ‡"""
        edge_types = {}
        
        for edge in self.graph_data["edges"]:
            edge_type = edge["relation_type"]
            edge_types[edge_type] = edge_types.get(edge_type, 0) + 1
        
        return {
            "type_distribution": edge_types,
            "average_confidence": sum(edge["confidence"] for edge in self.graph_data["edges"]) / len(self.graph_data["edges"])
        }
    
    async def _calculate_global_metrics(self) -> Dict[str, Any]:
        """è®¡ç®—å…¨å±€æŒ‡æ ‡"""
        return {
            "density": self._calculate_density(),
            "connected_components": self._find_connected_components(),
            "average_clustering": self._calculate_clustering_coefficient()
        }
    
    def _calculate_node_degree(self, node_id: str) -> int:
        """è®¡ç®—èŠ‚ç‚¹åº¦"""
        degree = 0
        for edge in self.graph_data["edges"]:
            if edge["source"] == node_id or edge["target"] == node_id:
                degree += 1
        return degree
    
    def _calculate_density(self) -> float:
        """è®¡ç®—å›¾å¯†åº¦"""
        n = len(self.graph_data["nodes"])
        if n <= 1:
            return 0.0
        
        max_edges = n * (n - 1)  # æœ‰å‘å›¾
        actual_edges = len(self.graph_data["edges"])
        
        return actual_edges / max_edges if max_edges > 0 else 0.0
    
    def _find_connected_components(self) -> int:
        """æŸ¥æ‰¾è¿é€šåˆ†é‡"""
        # ç®€å•çš„è¿é€šåˆ†é‡è®¡ç®—
        visited = set()
        components = 0
        
        for node in self.graph_data["nodes"]:
            if node["id"] not in visited:
                components += 1
                self._dfs(node["id"], visited)
        
        return components
    
    def _dfs(self, node_id: str, visited: Set[str]):
        """æ·±åº¦ä¼˜å…ˆæœç´¢"""
        visited.add(node_id)
        
        for edge in self.graph_data["edges"]:
            if edge["source"] == node_id:
                neighbor = edge["target"]
                if neighbor not in visited:
                    self._dfs(neighbor, visited)
            elif edge["target"] == node_id:
                neighbor = edge["source"]
                if neighbor not in visited:
                    self._dfs(neighbor, visited)
    
    def _calculate_clustering_coefficient(self) -> float:
        """è®¡ç®—èšç±»ç³»æ•°"""
        # ç®€åŒ–çš„èšç±»ç³»æ•°è®¡ç®—
        total_coefficient = 0.0
        node_count = 0
        
        for node in self.graph_data["nodes"]:
            neighbors = self._get_node_neighbors(node["id"])
            if len(neighbors) < 2:
                continue
            
            # è®¡ç®—é‚»å±…ä¹‹é—´çš„è¿æ¥æ•°
            possible_edges = len(neighbors) * (len(neighbors) - 1)
            actual_edges = 0
            
            for i, neighbor1 in enumerate(neighbors):
                for neighbor2 in neighbors[i+1:]:
                    if self._are_connected(neighbor1, neighbor2):
                        actual_edges += 1
            
            if possible_edges > 0:
                coefficient = actual_edges / possible_edges
                total_coefficient += coefficient
                node_count += 1
        
        return total_coefficient / node_count if node_count > 0 else 0.0
    
    def _get_node_neighbors(self, node_id: str) -> List[str]:
        """è·å–èŠ‚ç‚¹é‚»å±…"""
        neighbors = []
        for edge in self.graph_data["edges"]:
            if edge["source"] == node_id:
                neighbors.append(edge["target"])
            elif edge["target"] == node_id:
                neighbors.append(edge["source"])
        return neighbors
    
    def _are_connected(self, node1: str, node2: str) -> bool:
        """åˆ¤æ–­ä¸¤ä¸ªèŠ‚ç‚¹æ˜¯å¦ç›´æ¥è¿æ¥"""
        for edge in self.graph_data["edges"]:
            if ((edge["source"] == node1 and edge["target"] == node2) or
                (edge["source"] == node2 and edge["target"] == node1)):
                return True
        return False
    
    async def _generate_visualization(self) -> Dict[str, Any]:
        """ç”Ÿæˆå¯è§†åŒ–æ•°æ®"""
        # ç”Ÿæˆç”¨äºå¯è§†åŒ–çš„æ•°æ®æ ¼å¼
        visualization_data = {
            "nodes": [
                {
                    "id": node["id"],
                    "label": node["name"],
                    "type": node["type"],
                    "size": self._calculate_node_degree(node["id"]) * 10 + 10,
                    "color": self._get_node_color(node["type"])
                }
                for node in self.graph_data["nodes"]
            ],
            "edges": [
                {
                    "from": edge["source"],
                    "to": edge["target"],
                    "label": edge["relation_type"],
                    "width": edge["confidence"] * 5,
                    "color": self._get_edge_color(edge["relation_type"])
                }
                for edge in self.graph_data["edges"]
            ]
        }
        
        return visualization_data
    
    def _get_node_color(self, node_type: str) -> str:
        """è·å–èŠ‚ç‚¹é¢œè‰²"""
        color_map = {
            "person": "#FF6B6B",
            "organization": "#4ECDC4", 
            "location": "#45B7D1",
            "concept": "#96CEB4",
            "event": "#FFEAA7",
            "product": "#DDA0DD"
        }
        return color_map.get(node_type, "#95A5A6")
    
    def _get_edge_color(self, relation_type: str) -> str:
        """è·å–è¾¹é¢œè‰²"""
        color_map = {
            "related_to": "#BDC3C7",
            "part_of": "#3498DB",
            "located_in": "#E74C3C",
            "works_for": "#2ECC71",
            "influences": "#F39C12"
        }
        return color_map.get(relation_type, "#95A5A6")

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    builder = KnowledgeGraphBuilder()
    
    # ç¤ºä¾‹æ–‡æ¡£
    documents = [
        {
            "id": "doc_1",
            "title": "äººå·¥æ™ºèƒ½å‘å±•å²",
            "content": "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„é‡è¦åˆ†æ”¯ï¼Œæœºå™¨å­¦ä¹ æ˜¯AIçš„æ ¸å¿ƒæŠ€æœ¯ä¹‹ä¸€ã€‚æ·±åº¦å­¦ä¹ åœ¨è¿‘å¹´æ¥å–å¾—äº†é‡å¤§çªç ´ã€‚"
        },
        {
            "id": "doc_2", 
            "title": "æœºå™¨å­¦ä¹ åº”ç”¨",
            "content": "æœºå™¨å­¦ä¹ åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰ç­‰é¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨ã€‚ç¥ç»ç½‘ç»œæ˜¯æœºå™¨å­¦ä¹ çš„é‡è¦æ¨¡å‹ã€‚"
        },
        {
            "id": "doc_3",
            "title": "æ·±åº¦å­¦ä¹ è¿›å±•",
            "content": "æ·±åº¦å­¦ä¹ åŸºäºç¥ç»ç½‘ç»œï¼Œé€šè¿‡å¤šå±‚ç»“æ„å­¦ä¹ æ•°æ®çš„å¤æ‚æ¨¡å¼ã€‚å·ç§¯ç¥ç»ç½‘ç»œåœ¨å›¾åƒè¯†åˆ«ä¸­è¡¨ç°å‡ºè‰²ã€‚"
        }
    ]
    
    try:
        # æ„å»ºçŸ¥è¯†å›¾è°±
        result = await builder.build_knowledge_graph(documents)
        
        print("âœ… çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆ!")
        print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"  èŠ‚ç‚¹æ•°: {result['statistics']['total_nodes']}")
        print(f"  è¾¹æ•°: {result['statistics']['total_edges']}")
        print(f"  å›¾å¯†åº¦: {result['statistics']['density']:.3f}")
        print(f"  è¿é€šåˆ†é‡: {result['statistics']['connected_components']}")
        
        print(f"\nğŸ¯ æ ¸å¿ƒèŠ‚ç‚¹:")
        for node_id, score in result['metrics']['node_metrics']['top_central_nodes'][:5]:
            print(f"  {node_id}: {score}")
        
        print(f"\nğŸ”— å…³ç³»ç±»å‹åˆ†å¸ƒ:")
        for rel_type, count in result['metrics']['edge_metrics']['type_distribution'].items():
            print(f"  {rel_type}: {count}")
        
    finally:
        await builder.knowledge_analyzer.cleanup()
        await builder.knowledge_manager.cleanup()

if __name__ == "__main__":
    asyncio.run(main())
```

è¿™äº›é›†æˆç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•åœ¨å®é™…åº”ç”¨ä¸­å°†å¤šä¸ªç»„ä»¶ç»„åˆä½¿ç”¨ï¼Œå®ç°å¤æ‚çš„ä¸šåŠ¡åœºæ™¯ã€‚æ¯ä¸ªç¤ºä¾‹éƒ½æä¾›äº†å®Œæ•´çš„å®ç°ä»£ç ï¼Œå¯ä»¥ç›´æ¥è¿è¡Œå’Œæ‰©å±•ã€‚
