# é«˜çº§åŠŸèƒ½ç¤ºä¾‹

è¿™äº›ç¤ºä¾‹å±•ç¤ºäº† AI Tech Innovation Suite çš„é«˜çº§åŠŸèƒ½å’Œå¤æ‚ç”¨ä¾‹ã€‚

## ç›®å½•

- [å®Œæ•´å’–å•¡æ²™é¾™æµç¨‹](#å®Œæ•´å’–å•¡æ²™é¾™æµç¨‹)
- [ä¼ä¸šçº§éƒ¨ç½²](#ä¼ä¸šçº§éƒ¨ç½²)
- [è‡ªå®šä¹‰æ‰©å±•](#è‡ªå®šä¹‰æ‰©å±•)
- [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
- [å®‰å…¨é…ç½®](#å®‰å…¨é…ç½®)

## å®Œæ•´å’–å•¡æ²™é¾™æµç¨‹

è¿™ä¸ªç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•æ„å»ºä¸€ä¸ªå®Œæ•´çš„AIå’–å•¡æ²™é¾™ç³»ç»Ÿï¼ŒåŒ…å«å¤šæ™ºèƒ½ä½“åä½œã€å®æ—¶äº¤äº’å’ŒçŸ¥è¯†æ²‰æ·€ã€‚

```python
# examples/advanced-features/coffee_salon_workflow.py
import asyncio
from datetime import datetime, timedelta
from packages.multi_agent_framework import AgentRegistry, HostAgent, ExpertAgent
from packages.knowledge_emergence import KnowledgeEmergenceAnalyzer
from packages.xiaohongshu_agent import XiaohongshuAgent

class CoffeeSalonWorkflow:
    """å’–å•¡æ²™é¾™å·¥ä½œæµç®¡ç†å™¨"""
    
    def __init__(self):
        self.registry = AgentRegistry()
        self.knowledge_analyzer = KnowledgeEmergenceAnalyzer()
        self.content_agent = XiaohongshuAgent()
        self.salon_session = None
    
    async def setup_salon(self, topic: str, duration: int = 60):
        """è®¾ç½®æ²™é¾™"""
        # åˆ›å»ºä¸»æŒäºº
        host = HostAgent(
            name="å’–å•¡æ²™é¾™ä¸»æŒäºº",
            role="åè°ƒè®¨è®º",
            system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å’–å•¡æ²™é¾™ä¸»æŒäººï¼Œå–„äºå¼•å¯¼è¯é¢˜å’Œåè°ƒè®¨è®º"
        )
        
        # åˆ›å»ºä¸“å®¶
        experts = [
            ExpertAgent(
                name="å’–å•¡ä¸“å®¶",
                specialty="å’–å•¡åˆ¶ä½œ",
                system_prompt="ä½ æ˜¯ä¸€ä½å’–å•¡åˆ¶ä½œä¸“å®¶ï¼Œåˆ†äº«ä¸“ä¸šçš„å’–å•¡çŸ¥è¯†å’Œç»éªŒ"
            ),
            ExpertAgent(
                name="å’–å•¡æ–‡åŒ–ä¸“å®¶", 
                specialty="å’–å•¡æ–‡åŒ–",
                system_prompt="ä½ æ˜¯ä¸€ä½å’–å•¡æ–‡åŒ–ä¸“å®¶ï¼Œäº†è§£å’–å•¡çš„å†å²å’Œæ–‡åŒ–å†…æ¶µ"
            )
        ]
        
        # æ³¨å†Œæ™ºèƒ½ä½“
        await self.registry.register_agent("host", host)
        for i, expert in enumerate(experts):
            await self.registry.register_agent(f"expert_{i}", expert)
        
        # åˆ›å»ºæ²™é¾™ä¼šè¯
        self.salon_session = {
            "topic": topic,
            "start_time": datetime.now(),
            "duration": duration,
            "participants": ["host"] + [f"expert_{i}" for i in range(len(experts))],
            "knowledge_items": [],
            "dialogue_log": []
        }
        
        print(f"âœ… å’–å•¡æ²™é¾™å·²è®¾ç½®å®Œæˆ")
        print(f"ğŸ“ ä¸»é¢˜: {topic}")
        print(f"â° æ—¶é•¿: {duration}åˆ†é’Ÿ")
        print(f"ğŸ‘¥ å‚ä¸è€…: {', '.join(self.salon_session['participants'])}")
    
    async def run_salon_session(self):
        """è¿è¡Œæ²™é¾™ä¼šè¯"""
        if not self.salon_session:
            raise ValueError("æ²™é¾™æœªè®¾ç½®ï¼Œè¯·å…ˆè°ƒç”¨setup_salon")
        
        topic = self.salon_session["topic"]
        
        # æ¬¢è¿ç¯èŠ‚
        welcome_msg = await self.registry.send_message(
            agent_name="host",
            message=f"æ¬¢è¿å¤§å®¶å‚åŠ ä»Šå¤©çš„å’–å•¡æ²™é¾™ï¼ä»Šå¤©æˆ‘ä»¬è¦è®¨è®ºçš„ä¸»é¢˜æ˜¯ï¼š{topic}"
        )
        self.salon_session["dialogue_log"].append({
            "agent": "host",
            "message": welcome_msg,
            "timestamp": datetime.now()
        })
        
        # ä¸“å®¶è®¨è®º
        discussion_topics = [
            f"è¯·{expert_name}åˆ†äº«å…³äº{topic}çš„ä¸“ä¸šè§è§£"
            for expert_name in self.salon_session["participants"][1:]
        ]
        
        for topic_msg in discussion_topics:
            response = await self.registry.send_message(
                agent_name="host",
                message=topic_msg
            )
            self.salon_session["dialogue_log"].append({
                "agent": "host",
                "message": response,
                "timestamp": datetime.now()
            })
            
            # æå–çŸ¥è¯†é¡¹
            knowledge_item = {
                "id": f"knowledge_{len(self.salon_session['knowledge_items']) + 1}",
                "content": response,
                "source": "å’–å•¡æ²™é¾™è®¨è®º",
                "timestamp": datetime.now(),
                "topic": topic
            }
            self.salon_session["knowledge_items"].append(knowledge_item)
        
        # æ€»ç»“ç¯èŠ‚
        summary_msg = await self.registry.send_message(
            agent_name="host",
            message="è¯·å¯¹ä»Šå¤©çš„è®¨è®ºè¿›è¡Œæ€»ç»“ï¼Œå¹¶æå–å…³é”®çŸ¥è¯†ç‚¹"
        )
        self.salon_session["dialogue_log"].append({
            "agent": "host", 
            "message": summary_msg,
            "timestamp": datetime.now()
        })
        
        return self.salon_session
    
    async def analyze_knowledge(self):
        """åˆ†ææ²™é¾™äº§ç”Ÿçš„çŸ¥è¯†"""
        if not self.salon_session["knowledge_items"]:
            return None
        
        print("ğŸ” å¼€å§‹çŸ¥è¯†åˆ†æ...")
        
        # è´¨é‡è¯„ä¼°
        quality_result = await self.knowledge_analyzer.assess_quality(
            self.salon_session["knowledge_items"]
        )
        
        # ä»·å€¼åˆ†æ
        value_result = await self.knowledge_analyzer.assess_value(
            self.salon_session["knowledge_items"]
        )
        
        # æ¨¡å¼è¯†åˆ«
        pattern_result = await self.knowledge_analyzer.recognize_patterns(
            self.salon_session["knowledge_items"]
        )
        
        analysis_result = {
            "quality": quality_result,
            "value": value_result,
            "patterns": pattern_result,
            "summary": self._generate_summary()
        }
        
        print("âœ… çŸ¥è¯†åˆ†æå®Œæˆ")
        return analysis_result
    
    def _generate_summary(self):
        """ç”Ÿæˆæ²™é¾™æ€»ç»“"""
        return {
            "total_knowledge_items": len(self.salon_session["knowledge_items"]),
            "main_topics": list(set(item["topic"] for item in self.salon_session["knowledge_items"])),
            "duration_minutes": (datetime.now() - self.salon_session["start_time"]).total_seconds() / 60,
            "participant_count": len(self.salon_session["participants"])
        }
    
    async def generate_content(self, analysis_result):
        """ç”Ÿæˆå†…å®¹"""
        if not analysis_result:
            return None
        
        # åŸºäºåˆ†æç»“æœç”Ÿæˆå†…å®¹
        content = await self.content_agent.generate_content(
            topic=self.salon_session["topic"],
            brand_info={
                "name": "AIå’–å•¡æ²™é¾™",
                "category": "çŸ¥è¯†åˆ†äº«"
            },
            content_type="knowledge_sharing",
            additional_context={
                "quality_score": analysis_result["quality"]["overall_score"],
                "key_insights": analysis_result["patterns"]["key_insights"]
            }
        )
        
        return content
    
    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        await self.registry.cleanup()
        self.knowledge_analyzer.cleanup()
        await self.content_agent.cleanup()

async def main():
    """ä¸»å‡½æ•°"""
    salon = CoffeeSalonWorkflow()
    
    try:
        # è®¾ç½®æ²™é¾™
        await salon.setup_salon("ç²¾å“å’–å•¡çš„çƒ˜ç„™å·¥è‰º", duration=45)
        
        # è¿è¡Œæ²™é¾™
        session_result = await salon.run_salon_session()
        
        # åˆ†æçŸ¥è¯†
        analysis_result = await salon.analyze_knowledge()
        
        # ç”Ÿæˆå†…å®¹
        content = await salon.generate_content(analysis_result)
        
        # è¾“å‡ºç»“æœ
        print("\n" + "="*50)
        print("ğŸ‰ å’–å•¡æ²™é¾™å®Œæˆï¼")
        print("="*50)
        
        print(f"\nğŸ“Š çŸ¥è¯†åˆ†æç»“æœ:")
        if analysis_result:
            print(f"  è´¨é‡è¯„åˆ†: {analysis_result['quality']['overall_score']:.2f}")
            print(f"  ä»·å€¼è¯„ä¼°: {analysis_result['value']['overall_value']}")
            print(f"  è¯†åˆ«æ¨¡å¼: {len(analysis_result['patterns']['patterns'])}ä¸ª")
        
        print(f"\nğŸ“ ç”Ÿæˆçš„å†…å®¹:")
        if content:
            print(f"  æ ‡é¢˜: {content['title']}")
            print(f"  æ ‡ç­¾: {', '.join(content['hashtags'])}")
        
        print(f"\nğŸ’¾ æ²™é¾™æ€»ç»“:")
        summary = session_result.get("summary", {})
        print(f"  çŸ¥è¯†é¡¹æ•°é‡: {summary.get('total_knowledge_items', 0)}")
        print(f"  ä¸»è¦è¯é¢˜: {', '.join(summary.get('main_topics', []))}")
        print(f"  å‚ä¸äººæ•°: {summary.get('participant_count', 0)}")
        
    finally:
        await salon.cleanup()

if __name__ == "__main__":
    asyncio.run(main())
```

## ä¼ä¸šçº§éƒ¨ç½²

å±•ç¤ºå¦‚ä½•åœ¨ä¼ä¸šç¯å¢ƒä¸­éƒ¨ç½²å’Œé…ç½®ç³»ç»Ÿã€‚

```yaml
# examples/advanced-features/enterprise-deployment/docker-compose.prod.yml
version: '3.8'

services:
  # åº”ç”¨è´Ÿè½½å‡è¡¡å™¨
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.prod.conf:/etc/nginx/nginx.conf
      - ./nginx/ssl:/etc/nginx/ssl
    depends_on:
      - app
    restart: unless-stopped

  # ä¸»åº”ç”¨ (å¤šå®ä¾‹)
  app:
    image: ai-tech-suite:latest
    deploy:
      replicas: 3
      resources:
        limits:
          memory: 1G
          cpus: '0.5'
    environment:
      - DATABASE_URL=postgresql://prod_user:${DB_PASSWORD}@postgres-cluster:5432/ai_suite
      - REDIS_URL=redis://redis-cluster:6379/0
      - ELASTICSEARCH_URL=http://elasticsearch:9200
      - KAFKA_BROKERS=kafka:9092
    depends_on:
      - postgres-cluster
      - redis-cluster
      - elasticsearch
      - kafka
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # PostgreSQLé›†ç¾¤
  postgres-cluster:
    image: postgres:15-alpine
    environment:
      - POSTGRES_DB=ai_suite
      - POSTGRES_USER=prod_user
      - POSTGRES_PASSWORD=${DB_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./scripts/init-db-prod.sql:/docker-entrypoint-initdb.d/init.sql
    command: >
      postgres 
      -c max_connections=200
      -c shared_buffers=256MB
      -c effective_cache_size=1GB
      -c maintenance_work_mem=64MB
    restart: unless-stopped

  # Redisé›†ç¾¤
  redis-cluster:
    image: redis:7-alpine
    command: redis-server --appendonly yes --maxmemory 512mb --maxmemory-policy allkeys-lru
    volumes:
      - redis_data:/data
    restart: unless-stopped

  # Elasticsearch
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.8.0
    environment:
      - discovery.type=single-node
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
    restart: unless-stopped

  # Kafka
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    volumes:
      - zookeeper_data:/var/lib/zookeeper/data

  kafka:
    image: confluentinc/cp-kafka:latest
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    volumes:
      - kafka_data:/var/lib/kafka/data

  # ç›‘æ§
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.prod.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
    restart: unless-stopped

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD}
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards
    restart: unless-stopped

volumes:
  postgres_data:
  redis_data:
  elasticsearch_data:
  kafka_data:
  zookeeper_data:
  prometheus_data:
  grafana_data:
```

## è‡ªå®šä¹‰æ‰©å±•

å±•ç¤ºå¦‚ä½•åˆ›å»ºè‡ªå®šä¹‰æ™ºèƒ½ä½“å’ŒåŠŸèƒ½æ¨¡å—ã€‚

```python
# examples/advanced-features/custom_agent.py
from packages.multi_agent_framework.core.agent import BaseAgent
from packages.multi_agent_framework.core.message import Message
from typing import Dict, Any, List

class CustomExpertAgent(BaseAgent):
    """è‡ªå®šä¹‰ä¸“å®¶æ™ºèƒ½ä½“"""
    
    def __init__(self, name: str, specialty: str, expertise_areas: List[str]):
        super().__init__(name, "expert")
        self.specialty = specialty
        self.expertise_areas = expertise_areas
        self.knowledge_base = {}
        self.conversation_history = []
    
    async def process_message(self, message: Message) -> Message:
        """å¤„ç†æ¥æ”¶åˆ°çš„æ¶ˆæ¯"""
        # è®°å½•å¯¹è¯å†å²
        self.conversation_history.append({
            "timestamp": message.timestamp,
            "sender": message.sender,
            "content": message.content,
            "type": "received"
        })
        
        # åˆ†ææ¶ˆæ¯å†…å®¹
        analysis = await self._analyze_message(message.content)
        
        # ç”Ÿæˆå›å¤
        response_content = await self._generate_response(message.content, analysis)
        
        # è®°å½•å›å¤å†å²
        self.conversation_history.append({
            "timestamp": message.timestamp,
            "sender": self.name,
            "content": response_content,
            "type": "sent"
        })
        
        return Message(
            sender=self.name,
            receiver=message.sender,
            content=response_content,
            message_type="response",
            metadata={
                "specialty": self.specialty,
                "confidence": analysis.get("confidence", 0.8),
                "expertise_areas": self.expertise_areas
            }
        )
    
    async def _analyze_message(self, content: str) -> Dict[str, Any]:
        """åˆ†ææ¶ˆæ¯å†…å®¹"""
        # ç®€å•çš„å…³é”®è¯åˆ†æ
        keywords = content.lower().split()
        matched_areas = [area for area in self.expertise_areas 
                        if any(keyword in area.lower() for keyword in keywords)]
        
        return {
            "matched_expertise": matched_areas,
            "confidence": min(len(matched_areas) / len(self.expertise_areas), 1.0),
            "complexity": "high" if len(content) > 200 else "medium"
        }
    
    async def _generate_response(self, content: str, analysis: Dict[str, Any]) -> str:
        """ç”Ÿæˆå›å¤"""
        if not analysis["matched_expertise"]:
            return f"æŠ±æ­‰ï¼Œæˆ‘åœ¨{self.specialty}é¢†åŸŸæœ‰ä¸“é•¿ï¼Œä½†æ‚¨çš„é—®é¢˜ä¼¼ä¹ä¸åœ¨æˆ‘çš„ä¸“ä¸šèŒƒå›´å†…ã€‚æˆ‘å¯ä»¥ä¸ºæ‚¨æ¨èç›¸å…³çš„ä¸“å®¶ã€‚"
        
        # åŸºäºä¸“ä¸šé¢†åŸŸç”Ÿæˆå›å¤
        response_parts = []
        response_parts.append(f"ä½œä¸º{self.specialty}ä¸“å®¶ï¼Œæˆ‘çš„è§‚ç‚¹æ˜¯ï¼š")
        
        for area in analysis["matched_expertise"]:
            response_parts.append(f"\nå…³äº{area}ï¼š")
            response_parts.append(f"åŸºäºæˆ‘çš„ä¸“ä¸šçŸ¥è¯†ï¼Œæˆ‘å»ºè®®...")
        
        return "\n".join(response_parts)
    
    def add_knowledge(self, topic: str, knowledge: str):
        """æ·»åŠ çŸ¥è¯†åˆ°çŸ¥è¯†åº“"""
        if topic not in self.knowledge_base:
            self.knowledge_base[topic] = []
        self.knowledge_base[topic].append(knowledge)
    
    def get_expertise_summary(self) -> Dict[str, Any]:
        """è·å–ä¸“ä¸šèƒ½åŠ›æ‘˜è¦"""
        return {
            "name": self.name,
            "specialty": self.specialty,
            "expertise_areas": self.expertise_areas,
            "knowledge_topics": list(self.knowledge_base.keys()),
            "conversation_count": len(self.conversation_history),
            "expertise_level": "expert"  # å¯ä»¥æ ¹æ®ç»éªŒåŠ¨æ€è°ƒæ•´
        }

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    # åˆ›å»ºè‡ªå®šä¹‰ä¸“å®¶
    coffee_expert = CustomExpertAgent(
        name="å’–å•¡å·¥è‰ºä¸“å®¶",
        specialty="å’–å•¡çƒ˜ç„™ä¸åˆ¶ä½œ",
        expertise_areas=["å’–å•¡çƒ˜ç„™", "å’–å•¡ç ”ç£¨", "èƒå–æŠ€å·§", "å’–å•¡å“é‰´"]
    )
    
    # æ·»åŠ ä¸“ä¸šçŸ¥è¯†
    coffee_expert.add_knowledge("çƒ˜ç„™ç¨‹åº¦", "æµ…çƒ˜ç„™ä¿ç•™é…¸è´¨ï¼Œæ·±çƒ˜ç„™å¢åŠ è‹¦å‘³")
    coffee_expert.add_knowledge("ç ”ç£¨ç²—ç»†", "æ„å¼å’–å•¡éœ€è¦ç»†ç ”ç£¨ï¼Œæ‰‹å†²éœ€è¦ä¸­ç­‰ç²—ç»†")
    
    # æ¨¡æ‹Ÿå¯¹è¯
    from packages.multi_agent_framework.core.message import Message
    
    question = Message(
        sender="user",
        receiver="coffee_expert", 
        content="æˆ‘æƒ³äº†è§£æ‰‹å†²å’–å•¡çš„ç ”ç£¨ç²—ç»†åº”è¯¥å¦‚ä½•é€‰æ‹©ï¼Ÿ",
        message_type="question"
    )
    
    response = await coffee_expert.process_message(question)
    print(f"ä¸“å®¶å›å¤: {response.content}")
    
    # è·å–ä¸“å®¶èƒ½åŠ›æ‘˜è¦
    summary = coffee_expert.get_expertise_summary()
    print(f"ä¸“å®¶æ‘˜è¦: {summary}")

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
```

## æ€§èƒ½ä¼˜åŒ–

å±•ç¤ºç³»ç»Ÿæ€§èƒ½ä¼˜åŒ–çš„é…ç½®å’Œæœ€ä½³å®è·µã€‚

```python
# examples/advanced-features/performance_optimization.py
import asyncio
import time
from concurrent.futures import ThreadPoolExecutor
from typing import List, Dict, Any
import aioredis
from packages.multi_agent_framework.core.registry import AgentRegistry

class PerformanceOptimizer:
    """æ€§èƒ½ä¼˜åŒ–å™¨"""
    
    def __init__(self):
        self.redis_pool = None
        self.executor = ThreadPoolExecutor(max_workers=10)
        self.cache = {}
        self.metrics = {
            "requests": 0,
            "cache_hits": 0,
            "cache_misses": 0,
            "avg_response_time": 0.0
        }
    
    async def setup_optimizations(self):
        """è®¾ç½®æ€§èƒ½ä¼˜åŒ–"""
        # è¿æ¥æ± é…ç½®
        self.redis_pool = aioredis.ConnectionPool.from_url(
            "redis://localhost:6379/0",
            max_connections=20,
            retry_on_timeout=True
        )
        
        print("âœ… æ€§èƒ½ä¼˜åŒ–é…ç½®å®Œæˆ")
    
    async def optimize_agent_responses(self, agents: List[str], queries: List[str]):
        """ä¼˜åŒ–æ™ºèƒ½ä½“å“åº”æ€§èƒ½"""
        start_time = time.time()
        
        # å¹¶å‘å¤„ç†æŸ¥è¯¢
        tasks = []
        for query in queries:
            for agent in agents:
                task = self._cached_agent_call(agent, query)
                tasks.append(task)
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        end_time = time.time()
        total_time = end_time - start_time
        
        print(f"âš¡ æ€§èƒ½ç»Ÿè®¡:")
        print(f"  æ€»æŸ¥è¯¢æ•°: {len(queries) * len(agents)}")
        print(f"  æ€»è€—æ—¶: {total_time:.2f}ç§’")
        print(f"  å¹³å‡å“åº”æ—¶é—´: {total_time / len(tasks):.3f}ç§’")
        print(f"  ååé‡: {len(tasks) / total_time:.1f} è¯·æ±‚/ç§’")
        print(f"  ç¼“å­˜å‘½ä¸­ç‡: {self.metrics['cache_hits'] / self.metrics['requests'] * 100:.1f}%")
        
        return results
    
    async def _cached_agent_call(self, agent_name: str, query: str):
        """å¸¦ç¼“å­˜çš„æ™ºèƒ½ä½“è°ƒç”¨"""
        self.metrics["requests"] += 1
        
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = f"{agent_name}:{hash(query)}"
        
        # æ£€æŸ¥ç¼“å­˜
        if cache_key in self.cache:
            self.metrics["cache_hits"] += 1
            return self.cache[cache_key]
        
        self.metrics["cache_misses"] += 1
        
        # æ¨¡æ‹Ÿæ™ºèƒ½ä½“è°ƒç”¨
        start_time = time.time()
        result = await self._simulate_agent_call(agent_name, query)
        end_time = time.time()
        
        response_time = end_time - start_time
        
        # æ›´æ–°å¹³å‡å“åº”æ—¶é—´
        self.metrics["avg_response_time"] = (
            (self.metrics["avg_response_time"] * (self.metrics["requests"] - 1) + response_time) 
            / self.metrics["requests"]
        )
        
        # ç¼“å­˜ç»“æœ
        self.cache[cache_key] = result
        
        return result
    
    async def _simulate_agent_call(self, agent_name: str, query: str):
        """æ¨¡æ‹Ÿæ™ºèƒ½ä½“è°ƒç”¨ï¼ˆå®é™…åº”ç”¨ä¸­æ›¿æ¢ä¸ºçœŸå®è°ƒç”¨ï¼‰"""
        # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
        await asyncio.sleep(0.1)
        
        return {
            "agent": agent_name,
            "query": query,
            "response": f"æ¥è‡ª{agent_name}çš„å›å¤: {query[:50]}...",
            "timestamp": time.time()
        }
    
    async def batch_process_knowledge(self, knowledge_items: List[Dict[str, Any]]):
        """æ‰¹é‡å¤„ç†çŸ¥è¯†é¡¹"""
        print(f"ğŸ”„ å¼€å§‹æ‰¹é‡å¤„ç†{len(knowledge_items)}ä¸ªçŸ¥è¯†é¡¹...")
        
        start_time = time.time()
        
        # åˆ†æ‰¹å¤„ç†
        batch_size = 10
        batches = [knowledge_items[i:i + batch_size] 
                  for i in range(0, len(knowledge_items), batch_size)]
        
        results = []
        for i, batch in enumerate(batches):
            print(f"  å¤„ç†æ‰¹æ¬¡ {i+1}/{len(batches)}")
            
            # å¹¶å‘å¤„ç†æ‰¹æ¬¡
            batch_tasks = [self._process_knowledge_item(item) for item in batch]
            batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)
            results.extend(batch_results)
        
        end_time = time.time()
        total_time = end_time - start_time
        
        print(f"âœ… æ‰¹é‡å¤„ç†å®Œæˆ:")
        print(f"  å¤„ç†æ—¶é—´: {total_time:.2f}ç§’")
        print(f"  å¹³å‡æ¯é¡¹: {total_time / len(knowledge_items):.3f}ç§’")
        print(f"  ååé‡: {len(knowledge_items) / total_time:.1f} é¡¹/ç§’")
        
        return results
    
    async def _process_knowledge_item(self, item: Dict[str, Any]):
        """å¤„ç†å•ä¸ªçŸ¥è¯†é¡¹"""
        # æ¨¡æ‹Ÿå¤„ç†é€»è¾‘
        await asyncio.sleep(0.05)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
        
        return {
            "id": item.get("id"),
            "processed": True,
            "quality_score": 0.85,  # æ¨¡æ‹Ÿè¯„åˆ†
            "processing_time": 0.05
        }
    
    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self.redis_pool:
            await self.redis_pool.disconnect()
        self.executor.shutdown(wait=True)

# æ€§èƒ½æµ‹è¯•ç¤ºä¾‹
async def main():
    optimizer = PerformanceOptimizer()
    await optimizer.setup_optimizations()
    
    try:
        # æµ‹è¯•æ™ºèƒ½ä½“å“åº”æ€§èƒ½
        agents = ["agent1", "agent2", "agent3", "agent4", "agent5"]
        queries = [f"é—®é¢˜{i}" for i in range(100)]
        
        await optimizer.optimize_agent_responses(agents, queries)
        
        # æµ‹è¯•çŸ¥è¯†å¤„ç†æ€§èƒ½
        knowledge_items = [{"id": f"item_{i}", "content": f"çŸ¥è¯†å†…å®¹{i}"}
                          for i in range(50)]
        
        await optimizer.batch_process_knowledge(knowledge_items)
        
    finally:
        await optimizer.cleanup()

if __name__ == "__main__":
    asyncio.run(main())
```

## å®‰å…¨é…ç½®

å±•ç¤ºä¼ä¸šçº§å®‰å…¨é…ç½®å’Œæœ€ä½³å®è·µã€‚

```python
# examples/advanced-features/security_config.py
import os
import jwt
import hashlib
import secrets
from datetime import datetime, timedelta
from typing import Dict, Any, Optional
from cryptography.fernet import Fernet
from packages.multi_agent_framework.core.auth import AuthenticationManager

class SecurityManager:
    """å®‰å…¨ç®¡ç†å™¨"""
    
    def __init__(self):
        self.auth_manager = AuthenticationManager()
        self.encryption_key = self._get_or_create_encryption_key()
        self.cipher = Fernet(self.encryption_key)
        self.session_tokens = {}
        self.failed_attempts = {}
        self.rate_limits = {}
    
    def _get_or_create_encryption_key(self) -> bytes:
        """è·å–æˆ–åˆ›å»ºåŠ å¯†å¯†é’¥"""
        key_file = ".encryption_key"
        if os.path.exists(key_file):
            with open(key_file, "rb") as f:
                return f.read()
        else:
            key = Fernet.generate_key()
            with open(key_file, "wb") as f:
                f.write(key)
            os.chmod(key_file, 0o600)  # é™åˆ¶æ–‡ä»¶æƒé™
            return key
    
    def create_secure_config(self, config: Dict[str, Any]) -> str:
        """åˆ›å»ºå®‰å…¨é…ç½®"""
        # åŠ å¯†æ•æ„Ÿä¿¡æ¯
        sensitive_fields = ["api_key", "password", "secret", "token"]
        encrypted_config = {}
        
        for key, value in config.items():
            if any(sensitive in key.lower() for sensitive in sensitive_fields):
                encrypted_config[key] = self.cipher.encrypt(
                    str(value).encode()
                ).decode()
            else:
                encrypted_config[key] = value
        
        return self.cipher.encrypt(
            str(encrypted_config).encode()
        ).decode()
    
    def decrypt_config(self, encrypted_config: str) -> Dict[str, Any]:
        """è§£å¯†é…ç½®"""
        decrypted_data = self.cipher.decrypt(encrypted_config.encode()).decode()
        config = eval(decrypted_data)  # æ³¨æ„ï¼šå®é™…ä½¿ç”¨ä¸­åº”ä½¿ç”¨json.loads
        
        # è§£å¯†æ•æ„Ÿå­—æ®µ
        sensitive_fields = ["api_key", "password", "secret", "token"]
        for key, value in config.items():
            if any(sensitive in key.lower() for sensitive in sensitive_fields):
                try:
                    config[key] = self.cipher.decrypt(value.encode()).decode()
                except:
                    pass  # å¦‚æœè§£å¯†å¤±è´¥ï¼Œä¿æŒåŸå€¼
        
        return config
    
    def create_session_token(self, user_id: str, permissions: list) -> str:
        """åˆ›å»ºä¼šè¯ä»¤ç‰Œ"""
        payload = {
            "user_id": user_id,
            "permissions": permissions,
            "exp": datetime.utcnow() + timedelta(hours=24),
            "iat": datetime.utcnow(),
            "jti": secrets.token_hex(16)  # å”¯ä¸€æ ‡è¯†ç¬¦
        }
        
        token = jwt.encode(payload, self.encryption_key, algorithm="HS256")
        self.session_tokens[token] = payload
        
        return token
    
    def validate_session_token(self, token: str) -> Optional[Dict[str, Any]]:
        """éªŒè¯ä¼šè¯ä»¤ç‰Œ"""
        try:
            payload = jwt.decode(token, self.encryption_key, algorithms=["HS256"])
            
            # æ£€æŸ¥ä»¤ç‰Œæ˜¯å¦åœ¨ä¼šè¯åˆ—è¡¨ä¸­
            if token not in self.session_tokens:
                return None
            
            # æ£€æŸ¥ä»¤ç‰Œæ˜¯å¦è¿‡æœŸ
            if datetime.utcnow() > datetime.fromtimestamp(payload["exp"]):
                del self.session_tokens[token]
                return None
            
            return payload
            
        except jwt.InvalidTokenError:
            return None
    
    def check_rate_limit(self, identifier: str, limit: int = 100, window: int = 3600):
        """æ£€æŸ¥é€Ÿç‡é™åˆ¶"""
        now = datetime.utcnow()
        
        if identifier not in self.rate_limits:
            self.rate_limits[identifier] = []
        
        # æ¸…ç†è¿‡æœŸè®°å½•
        self.rate_limits[identifier] = [
            timestamp for timestamp in self.rate_limits[identifier]
            if now - timestamp < timedelta(seconds=window)
        ]
        
        # æ£€æŸ¥æ˜¯å¦è¶…è¿‡é™åˆ¶
        if len(self.rate_limits[identifier]) >= limit:
            return False
        
        # è®°å½•å½“å‰è¯·æ±‚
        self.rate_limits[identifier].append(now)
        return True
    
    def check_brute_force(self, identifier: str, max_attempts: int = 5, lockout_time: int = 900):
        """æ£€æŸ¥æš´åŠ›ç ´è§£"""
        now = datetime.utcnow()
        
        if identifier not in self.failed_attempts:
            self.failed_attempts[identifier] = []
        
        # æ¸…ç†è¿‡æœŸè®°å½•
        self.failed_attempts[identifier] = [
            timestamp for timestamp in self.failed_attempts[identifier]
            if now - timestamp < timedelta(seconds=lockout_time)
        ]
        
        # æ£€æŸ¥æ˜¯å¦è¢«é”å®š
        if len(self.failed_attempts[identifier]) >= max_attempts:
            return False
        
        return True
    
    def record_failed_attempt(self, identifier: str):
        """è®°å½•å¤±è´¥å°è¯•"""
        if identifier not in self.failed_attempts:
            self.failed_attempts[identifier] = []
        
        self.failed_attempts[identifier].append(datetime.utcnow())
    
    def hash_password(self, password: str, salt: Optional[str] = None) -> tuple:
        """å®‰å…¨å¯†ç å“ˆå¸Œ"""
        if salt is None:
            salt = secrets.token_hex(32)
        
        password_hash = hashlib.pbkdf2_hmac(
            "sha256",
            password.encode("utf-8"),
            salt.encode("utf-8"),
            100000  # è¿­ä»£æ¬¡æ•°
        )
        
        return salt, password_hash.hex()
    
    def verify_password(self, password: str, salt: str, password_hash: str) -> bool:
        """éªŒè¯å¯†ç """
        _, computed_hash = self.hash_password(password, salt)
        return computed_hash == password_hash

# å®‰å…¨é…ç½®ç¤ºä¾‹
async def main():
    security = SecurityManager()
    
    # åˆ›å»ºå®‰å…¨é…ç½®
    config = {
        "database_url": "postgresql://user:pass@localhost/db",
        "api_key": "secret_api_key_123",
        "redis_url": "redis://localhost:6379",
        "debug": False
    }
    
    encrypted_config = security.create_secure_config(config)
    print(f"åŠ å¯†é…ç½®: {encrypted_config[:50]}...")
    
    decrypted_config = security.decrypt_config(encrypted_config)
    print(f"è§£å¯†é…ç½®: {decrypted_config}")
    
    # åˆ›å»ºä¼šè¯ä»¤ç‰Œ
    token = security.create_session_token("user123", ["read", "write"])
    print(f"ä¼šè¯ä»¤ç‰Œ: {token}")
    
    # éªŒè¯ä»¤ç‰Œ
    payload = security.validate_session_token(token)
    print(f"ä»¤ç‰Œè½½è·: {payload}")
    
    # é€Ÿç‡é™åˆ¶æµ‹è¯•
    for i in range(105):
        if not security.check_rate_limit("user123", limit=100):
            print(f"é€Ÿç‡é™åˆ¶è§¦å‘åœ¨ç¬¬{i+1}æ¬¡è¯·æ±‚")
            break
    
    print("âœ… å®‰å…¨é…ç½®æµ‹è¯•å®Œæˆ")

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
```

è¿™äº›é«˜çº§ç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•åœ¨å®é™…é¡¹ç›®ä¸­åº”ç”¨ AI Tech Innovation Suite çš„é«˜çº§åŠŸèƒ½ã€‚æ¯ä¸ªç¤ºä¾‹éƒ½å¯ä»¥ç‹¬ç«‹è¿è¡Œï¼Œå¹¶æä¾›äº†å®Œæ•´çš„ä»£ç å®ç°å’Œè¯¦ç»†æ³¨é‡Šã€‚
